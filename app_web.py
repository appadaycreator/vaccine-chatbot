import os

import ollama
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®å¾Œå¥åº·è¦³å¯Ÿã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", page_icon="ğŸ¥")
st.title("ğŸ¥ å¥åº·è¦³å¯Ÿã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
st.caption("åšåŠ´çœã®å®Ÿæ–½è¦é ˜ã«åŸºã¥ã„ãŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

# å…è²¬ï¼ˆUIã«å¸¸è¨­ï¼‰ï¼‹ç›¸è«‡å°ç·š
st.info(
    "ã“ã®ãƒ„ãƒ¼ãƒ«ã¯è³‡æ–™ã«åŸºã¥ãæƒ…å ±æä¾›ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€è¨ºæ–­ã‚„æ²»ç™‚ã®ä»£æ›¿ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    "ä½“èª¿ãŒæ‚ªã„ãƒ»ä¸å®‰ãŒå¼·ã„å ´åˆã¯ã€æ¥ç¨®ã‚’å—ã‘ãŸåŒ»ç™‚æ©Ÿé–¢ã‚„è‡ªæ²»ä½“ã®äºˆé˜²æ¥ç¨®ç›¸è«‡çª“å£ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚\n"
    "ç·Šæ€¥æ€§ãŒç–‘ã‚ã‚Œã‚‹å ´åˆï¼ˆå‘¼å¸ãŒè‹¦ã—ã„ã€æ„è­˜ãŒã‚‚ã†ã‚ã†ç­‰ï¼‰ã¯ 119ï¼ˆæ•‘æ€¥ï¼‰ã‚’åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚"
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆè¨­å®šï¼‰
with st.sidebar:
    st.header("è¨­å®š")
    llm_model = st.selectbox("å›ç­”ãƒ¢ãƒ‡ãƒ«", ["gemma2", "llama3.1"], index=0)
    k = st.slider("æ¤œç´¢ã®å¼·ã•ï¼ˆkå€¤ï¼‰", min_value=1, max_value=10, value=3, step=1)
    st.caption("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: `nomic-embed-text`ï¼ˆå›ºå®šï¼‰")
    st.caption("PDFã¯ã‚µãƒ¼ãƒãƒ¼å´ã§ `./pdfs/`ï¼ˆç’°å¢ƒå¤‰æ•° `PDF_DIR`ï¼‰ã«é…ç½®ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚")


def _no_sources_answer(question: str) -> str:
    q = (question or "").strip()
    qline = f"ï¼ˆè³ªå•: {q}ï¼‰" if q else ""
    return (
        "çµè«–:\n"
        "è³‡æ–™ã«è¨˜è¼‰ãŒãªã„ãŸã‚ã€ã“ã®è³‡æ–™ã«åŸºã¥ãå›ç­”ã¯ã§ãã¾ã›ã‚“ã€‚"
        f"{qline}\n\n"
        "æ ¹æ‹ :\n"
        "- è³‡æ–™ã«ãªã„ï¼ˆå‚ç…§PDFã‹ã‚‰è©²å½“ç®‡æ‰€ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰\n\n"
        "ç›¸è«‡å…ˆ:\n"
        "- æ¥ç¨®ã‚’å—ã‘ãŸåŒ»ç™‚æ©Ÿé–¢\n"
        "- ãŠä½ã¾ã„ã®è‡ªæ²»ä½“ã®äºˆé˜²æ¥ç¨®ç›¸è«‡çª“å£\n"
        "- ç—‡çŠ¶ãŒå¼·ã„ï¼æ€¥ã«æ‚ªåŒ–ã—ãŸï¼ç·Šæ€¥æ€§ãŒç–‘ã‚ã‚Œã‚‹å ´åˆ: 119ï¼ˆæ•‘æ€¥ï¼‰\n"
    )


def _build_answer_prompt(*, question: str, context: str) -> str:
    return f"""
ã‚ãªãŸã¯åŒ»ç™‚æƒ…å ±ã®æ–‡è„ˆã§ã€åšåŠ´çœç­‰ã®é…å¸ƒè³‡æ–™ï¼ˆä¸‹ã®ã€è³‡æ–™ã€‘ï¼‰ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã§è£œå®Œã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ã€è³‡æ–™ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯ã€Œè³‡æ–™ã«ãªã„ã€ã¨æ˜ç¢ºã«è¿°ã¹ã¦ãã ã•ã„ã€‚

å¿…ãšæ¬¡ã®3ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã ã‘ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆè¦‹å‡ºã—åã¯å›ºå®šï¼‰:
çµè«–:
æ ¹æ‹ :
ç›¸è«‡å…ˆ:

ãƒ«ãƒ¼ãƒ«:
- ã€è³‡æ–™ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ãªã„å†…å®¹ã‚’æ–­å®šã—ãªã„ï¼ˆæ›–æ˜§ã«ãã‚Œã£ã½ãè¨€ã‚ãªã„ï¼‰
- ã€Œæ ¹æ‹ ã€ã«ã¯ã€ã€è³‡æ–™ã€‘ã‹ã‚‰è©²å½“ç®‡æ‰€ã‚’å¼•ç”¨/è¦ç´„ã—ã¦ç®‡æ¡æ›¸ãã§ç¤ºã™
- ã€Œç›¸è«‡å…ˆã€ã¯å¿…ãš1ã¤ä»¥ä¸Šã€‚ç·Šæ€¥æ€§ãŒç–‘ã‚ã‚Œã‚‹å ´åˆã¯æ•‘æ€¥ï¼ˆ119ï¼‰ã‚‚å«ã‚ã‚‹
- ä½™è¨ˆãªå…è²¬æ–‡ã‚„è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ³¨æ„/è£œè¶³ãªã©ï¼‰ã¯å‡ºã•ãªã„ï¼ˆUIå´ã§å¸¸è¨­ã™ã‚‹ãŸã‚ï¼‰

ã€è³‡æ–™ã€‘:
{context}

è³ªå•: {question}
""".strip()


def _normalize_docs_source(docs, source_label: str):
    for d in docs:
        d.metadata = dict(d.metadata or {})
        d.metadata["source"] = source_label
    return docs


@st.cache_resource(show_spinner=False)
def _build_vectorstore_from_paths(paths: list[str], signature: str):
    # signature ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼å®‰å®šåŒ–ã®ãŸã‚
    chunks = []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    for p in paths:
        loader = PyPDFLoader(p)
        docs = loader.load()
        docs = _normalize_docs_source(docs, os.path.basename(p))
        chunks.extend([c for c in splitter.split_documents(docs) if (c.page_content or "").strip()])
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    return Chroma.from_documents(documents=chunks, embedding=embeddings)


def _list_pdf_paths() -> list[str]:
    pdf_dir = os.environ.get("PDF_DIR", "./pdfs")
    pdf_path = os.environ.get("PDF_PATH", "vaccine_manual.pdf")
    paths: list[str] = []
    if pdf_path and os.path.exists(pdf_path) and pdf_path.lower().endswith(".pdf"):
        paths.append(pdf_path)
    try:
        if pdf_dir and os.path.isdir(pdf_dir):
            for f in sorted(os.listdir(pdf_dir)):
                if f.lower().endswith(".pdf"):
                    paths.append(os.path.join(pdf_dir, f))
    except Exception:
        pass
    # é‡è¤‡é™¤å»
    uniq: list[str] = []
    seen: set[str] = set()
    for p in paths:
        ap = os.path.abspath(p)
        if ap in seen:
            continue
        seen.add(ap)
        uniq.append(p)
    return uniq


def _signature(paths: list[str]) -> str:
    parts: list[str] = []
    for p in paths:
        try:
            st_ = os.stat(p)
            parts.append(f"{os.path.abspath(p)}|{int(st_.st_size)}|{float(st_.st_mtime)}")
        except Exception:
            parts.append(f"{os.path.abspath(p)}|NA|NA")
    return "\n".join(sorted(parts))

paths = _list_pdf_paths()
try:
    if not paths:
        st.error("å‚ç…§ã™ã‚‹PDFãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚`vaccine_manual.pdf` ã¾ãŸã¯ `./pdfs/` ã«PDFã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    sig = _signature(paths)
    with st.spinner("PDFã‚’è§£æã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
        vectorstore = _build_vectorstore_from_paths(paths, sig)
    st.success(f"è³‡æ–™ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆ{len(paths)}ä»¶ï¼‰ã€‚")
except Exception as e:
    st.error(f"PDFã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}")
    st.stop()

# ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
col1, col2 = st.columns([1, 3])
with col1:
    if st.button("å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.messages = []
        st.rerun()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and message.get("sources"):
            sources = message["sources"]
            pages = []
            for s in sources:
                if s.get("page") is not None:
                    pages.append(f"{s.get('source','è³‡æ–™')} p.{s['page']}")
                else:
                    pages.append(f"{s.get('source','è³‡æ–™')}")
            st.markdown("**æ ¹æ‹ ï¼ˆå‚ç…§ãƒšãƒ¼ã‚¸ï¼‰**: " + " / ".join(pages))
            with st.expander("æ ¹æ‹ ã®æŠœç²‹ã‚’è¡¨ç¤º"):
                for s in sources:
                    title = f"{s.get('source','è³‡æ–™')}"
                    if s.get("page") is not None:
                        title += f" p.{s['page']}"
                    st.markdown(f"- {title}")
                    if s.get("excerpt"):
                        st.caption(s["excerpt"])


def _extract_sources(docs):
    sources = []
    seen = set()
    for d in docs:
        meta = d.metadata or {}
        src = meta.get("source") or "è³‡æ–™"
        page = meta.get("page")
        page_num = page + 1 if isinstance(page, int) else None
        key = (src, page_num)
        if key in seen:
            continue
        seen.add(key)
        excerpt = (d.page_content or "").strip().replace("\n", " ")
        if len(excerpt) > 400:
            excerpt = excerpt[:400] + "â€¦"
        sources.append({"source": src, "page": page_num, "excerpt": excerpt})
    return sources

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # RAGãƒ­ã‚¸ãƒƒã‚¯
    with st.chat_message("assistant"):
        with st.spinner("è³‡æ–™ã‚’ç¢ºèªä¸­..."):
            # æ¤œç´¢
            docs = vectorstore.similarity_search(prompt, k=k)
            context = "\n".join([doc.page_content for doc in docs])
            sources = _extract_sources(docs)

            # æ ¹æ‹ ãŒå–ã‚Œãªã„å ´åˆã¯ã€ç”Ÿæˆã›ãšã«å›ºå®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã™ï¼ˆæ–­å®š/hallucinationé˜²æ­¢ï¼‰
            if not sources:
                answer = _no_sources_answer(prompt)
            else:
                full_prompt = _build_answer_prompt(question=prompt, context=context)
                response = ollama.generate(model=llm_model, prompt=full_prompt)
                answer = response["response"]
            
            st.markdown(answer)
            if sources:
                pages = []
                for s in sources:
                    if s.get("page") is not None:
                        pages.append(f"{s.get('source','è³‡æ–™')} p.{s['page']}")
                    else:
                        pages.append(f"{s.get('source','è³‡æ–™')}")
                st.markdown("**æ ¹æ‹ ï¼ˆå‚ç…§ãƒšãƒ¼ã‚¸ï¼‰**: " + " / ".join(pages))
                with st.expander("æ ¹æ‹ ã®æŠœç²‹ã‚’è¡¨ç¤º"):
                    for s in sources:
                        title = f"{s.get('source','è³‡æ–™')}"
                        if s.get("page") is not None:
                            title += f" p.{s['page']}"
                        st.markdown(f"- {title}")
                        if s.get("excerpt"):
                            st.caption(s["excerpt"])

            st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})