import os
import re

import ollama
import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_community.vectorstores import Chroma
from langchain_text_splitters import RecursiveCharacterTextSplitter

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®å¾Œå¥åº·è¦³å¯Ÿã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", page_icon="ğŸ¥")
st.title("ğŸ¥ å¥åº·è¦³å¯Ÿã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
st.caption("åšåŠ´çœã®å®Ÿæ–½è¦é ˜ã«åŸºã¥ã„ãŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

# å…è²¬ï¼ˆUIã«å¸¸è¨­ï¼‰ï¼‹ç›¸è«‡å°ç·š
st.info(
    "ã“ã®ãƒ„ãƒ¼ãƒ«ã¯è³‡æ–™ã«åŸºã¥ãæƒ…å ±æä¾›ã‚’ç›®çš„ã¨ã—ã¦ãŠã‚Šã€è¨ºæ–­ã‚„æ²»ç™‚ã®ä»£æ›¿ã§ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚\n"
    "ä½“èª¿ãŒæ‚ªã„ãƒ»ä¸å®‰ãŒå¼·ã„å ´åˆã¯ã€æ¥ç¨®ã‚’å—ã‘ãŸåŒ»ç™‚æ©Ÿé–¢ã‚„è‡ªæ²»ä½“ã®äºˆé˜²æ¥ç¨®ç›¸è«‡çª“å£ã«ç›¸è«‡ã—ã¦ãã ã•ã„ã€‚\n"
    "ç·Šæ€¥æ€§ãŒç–‘ã‚ã‚Œã‚‹å ´åˆï¼ˆå‘¼å¸ãŒè‹¦ã—ã„ã€æ„è­˜ãŒã‚‚ã†ã‚ã†ç­‰ï¼‰ã¯ 119ï¼ˆæ•‘æ€¥ï¼‰ã‚’åˆ©ç”¨ã—ã¦ãã ã•ã„ã€‚"
)

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆè¨­å®šï¼‰
with st.sidebar:
    st.header("è¨­å®š")
    llm_model = st.selectbox("å›ç­”ãƒ¢ãƒ‡ãƒ«", ["gemma2", "llama3.1"], index=0)
    k = st.slider("æ¤œç´¢ã®å¼·ã•ï¼ˆkå€¤ï¼‰", min_value=1, max_value=10, value=3, step=1)
    st.caption("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: `nomic-embed-text`ï¼ˆå›ºå®š / æœªå°å…¥ãªã‚‰ `ollama pull nomic-embed-text`ï¼‰")
    st.caption("PDFã¯ã‚µãƒ¼ãƒãƒ¼å´ã§ `./pdfs/`ï¼ˆç’°å¢ƒå¤‰æ•° `PDF_DIR`ï¼‰ã«é…ç½®ã—ã¦åˆ©ç”¨ã—ã¾ã™ã€‚")
    st.divider()
    st.caption("æ“ä½œãƒ’ãƒ³ãƒˆ: ã€Œä¾‹ã€ãƒœã‚¿ãƒ³ã§è³ªå•ã‚’è‡ªå‹•é€ä¿¡ / ã€Œå†é€ã€ã§æœ€å¾Œã®è³ªå•ã‚’ã‚‚ã†ä¸€åº¦é€ã‚Œã¾ã™ã€‚")


def _no_sources_answer(question: str) -> str:
    q = (question or "").strip()
    qline = f"ï¼ˆè³ªå•: {q}ï¼‰" if q else ""
    return (
        "çµè«–:\n"
        "è³‡æ–™ã«è¨˜è¼‰ãŒãªã„ãŸã‚ã€ã“ã®è³‡æ–™ã«åŸºã¥ãå›ç­”ã¯ã§ãã¾ã›ã‚“ã€‚"
        f"{qline}\n\n"
        "æ ¹æ‹ :\n"
        "- è³‡æ–™ã«ãªã„ï¼ˆå‚ç…§PDFã‹ã‚‰è©²å½“ç®‡æ‰€ã‚’ç‰¹å®šã§ãã¾ã›ã‚“ã§ã—ãŸï¼‰\n\n"
        "ç›¸è«‡å…ˆ:\n"
        "- æ¥ç¨®ã‚’å—ã‘ãŸåŒ»ç™‚æ©Ÿé–¢\n"
        "- ãŠä½ã¾ã„ã®è‡ªæ²»ä½“ã®äºˆé˜²æ¥ç¨®ç›¸è«‡çª“å£\n"
        "- ç—‡çŠ¶ãŒå¼·ã„ï¼æ€¥ã«æ‚ªåŒ–ã—ãŸï¼ç·Šæ€¥æ€§ãŒç–‘ã‚ã‚Œã‚‹å ´åˆ: 119ï¼ˆæ•‘æ€¥ï¼‰\n"
    )


def _build_answer_prompt(*, question: str, context: str) -> str:
    return f"""
ã‚ãªãŸã¯åŒ»ç™‚æƒ…å ±ã®æ–‡è„ˆã§ã€åšåŠ´çœç­‰ã®é…å¸ƒè³‡æ–™ï¼ˆä¸‹ã®ã€è³‡æ–™ã€‘ï¼‰ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
æ¨æ¸¬ã‚„ä¸€èˆ¬è«–ã§è£œå®Œã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚ã€è³‡æ–™ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ãªã„ã“ã¨ã¯ã€Œè³‡æ–™ã«ãªã„ã€ã¨æ˜ç¢ºã«è¿°ã¹ã¦ãã ã•ã„ã€‚

å¿…ãšæ¬¡ã®3ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã ã‘ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ï¼ˆè¦‹å‡ºã—åã¯å›ºå®šï¼‰:
çµè«–:
æ ¹æ‹ :
ç›¸è«‡å…ˆ:

ãƒ«ãƒ¼ãƒ«:
- ã€è³‡æ–™ã€‘ã«æ›¸ã‹ã‚Œã¦ã„ãªã„å†…å®¹ã‚’æ–­å®šã—ãªã„ï¼ˆæ›–æ˜§ã«ãã‚Œã£ã½ãè¨€ã‚ãªã„ï¼‰
- ã€Œæ ¹æ‹ ã€ã«ã¯ã€ã€è³‡æ–™ã€‘ã‹ã‚‰è©²å½“ç®‡æ‰€ã‚’å¼•ç”¨/è¦ç´„ã—ã¦ç®‡æ¡æ›¸ãã§ç¤ºã™
- ã€Œç›¸è«‡å…ˆã€ã¯å¿…ãš1ã¤ä»¥ä¸Šã€‚ç·Šæ€¥æ€§ãŒç–‘ã‚ã‚Œã‚‹å ´åˆã¯æ•‘æ€¥ï¼ˆ119ï¼‰ã‚‚å«ã‚ã‚‹
- ä½™è¨ˆãªå…è²¬æ–‡ã‚„è¿½åŠ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆæ³¨æ„/è£œè¶³ãªã©ï¼‰ã¯å‡ºã•ãªã„ï¼ˆUIå´ã§å¸¸è¨­ã™ã‚‹ãŸã‚ï¼‰

ã€è³‡æ–™ã€‘:
{context}

è³ªå•: {question}
""".strip()


def _normalize_docs_source(docs, source_label: str):
    for d in docs:
        d.metadata = dict(d.metadata or {})
        d.metadata["source"] = source_label
    return docs


def _normalize_newlines(text: str) -> str:
    return (text or "").replace("\r\n", "\n").replace("\r", "\n")


def _clean_pdf_text(text: str) -> str:
    t = _normalize_newlines(text)
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    t = re.sub(r"(?<=\w)-\n(?=\w)", "", t)
    return t.strip()


def _strip_repeated_header_footer(docs: list, *, top_n: int = 2, bottom_n: int = 2, min_ratio: float = 0.6) -> None:
    pages = len(docs)
    if pages <= 1:
        return

    def _edge_lines(text: str):
        lines = [ln.strip() for ln in _normalize_newlines(text).split("\n")]
        lines = [ln for ln in lines if ln]
        return lines[:top_n], (lines[-bottom_n:] if lines else [])

    from collections import Counter

    top_counter = Counter()
    bottom_counter = Counter()
    for d in docs:
        top, bottom = _edge_lines(getattr(d, "page_content", "") or "")
        top_counter.update(top)
        bottom_counter.update(bottom)

    threshold = max(2, int(pages * min_ratio))

    def _pick(counter):
        out = set()
        for ln, c in counter.items():
            if c >= threshold and 2 <= len(ln) <= 80:
                out.add(ln)
        return out

    top_rm = _pick(top_counter)
    bottom_rm = _pick(bottom_counter)

    for d in docs:
        raw = _normalize_newlines(getattr(d, "page_content", "") or "")
        lines = [ln.rstrip() for ln in raw.split("\n")]
        i = 0
        while i < len(lines) and lines[i].strip() == "":
            i += 1
        for _ in range(top_n):
            if i < len(lines) and lines[i].strip() in top_rm:
                lines[i] = ""
                i += 1
            else:
                break
        j = len(lines) - 1
        while j >= 0 and lines[j].strip() == "":
            j -= 1
        for _ in range(bottom_n):
            if j >= 0 and lines[j].strip() in bottom_rm:
                lines[j] = ""
                j -= 1
            else:
                break
        d.page_content = _clean_pdf_text("\n".join(lines))


def _get_splitter() -> RecursiveCharacterTextSplitter:
    try:
        chunk_size = int(os.environ.get("CHUNK_SIZE", "900"))
    except Exception:
        chunk_size = 900
    try:
        chunk_overlap = int(os.environ.get("CHUNK_OVERLAP", "120"))
    except Exception:
        chunk_overlap = 120
    chunk_size = max(200, min(chunk_size, 5000))
    chunk_overlap = max(0, min(chunk_overlap, max(0, chunk_size - 1)))
    return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)


def _load_pdf_docs_best_effort(pdf_path: str):
    prefer = (os.environ.get("PDF_LOADER", "auto") or "auto").strip().lower()

    def _try_pymupdf():
        try:
            from langchain_community.document_loaders import PyMuPDFLoader  # type: ignore

            return PyMuPDFLoader(pdf_path).load()
        except Exception:
            return None

    if prefer in ("pymupdf", "fitz"):
        docs = _try_pymupdf()
        if docs is None:
            raise RuntimeError("PDF_LOADER=pymupdf ãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã™ãŒã€PyMuPDFLoaderï¼ˆpymupdfï¼‰ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return docs, "pymupdf"
    if prefer in ("pypdf", "pdf"):
        return PyPDFLoader(pdf_path).load(), "pypdf"

    docs = _try_pymupdf()
    if docs is not None:
        return docs, "pymupdf"
    return PyPDFLoader(pdf_path).load(), "pypdf"


def _make_excerpt(text: str, max_lines: int = 10, max_chars: int = 900) -> str:
    raw = _normalize_newlines(text).strip()
    if not raw:
        return ""
    lines = [ln.strip() for ln in raw.split("\n")]
    lines = [ln for ln in lines if ln]
    if not lines:
        return ""
    if len(lines) <= max_lines:
        picked = lines
    else:
        head_n = max(1, max_lines // 2)
        tail_n = max(1, max_lines - head_n - 1)
        picked = lines[:head_n] + ["â€¦"] + lines[-tail_n:]
    out: list[str] = []
    total = 0
    for ln in picked:
        if ln != "â€¦" and total + len(ln) + 1 > max_chars:
            break
        out.append(ln)
        total += len(ln) + 1
        if total >= max_chars:
            break
    return "\n".join(out).strip()


def _model_names_from_ollama_list(payload) -> list[str]:
    if not isinstance(payload, dict):
        return []
    models = payload.get("models")
    if not isinstance(models, list):
        return []
    out: list[str] = []
    for m in models:
        if isinstance(m, dict) and isinstance(m.get("name"), str):
            out.append(m["name"])
    return out


def _has_model(model_names: list[str], wanted: str) -> bool:
    w = (wanted or "").strip()
    if not w:
        return False
    return any(n == w or n.startswith(w + ":") for n in model_names)


def _ensure_embedding_model(model: str = "nomic-embed-text") -> None:
    try:
        info = ollama.list()
    except Exception as e:
        raise RuntimeError(
            "Ollama ã«æ¥ç¶šã§ãã¾ã›ã‚“ï¼ˆæœªèµ·å‹•ã®å¯èƒ½æ€§ï¼‰ã€‚\n"
            "å¯¾å‡¦:\n"
            "- Ollama ãŒèµ·å‹•ã—ã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ï¼ˆä¾‹: brew services start ollamaï¼‰"
        ) from e
    names = _model_names_from_ollama_list(info)
    if not _has_model(names, model):
        raise RuntimeError(
            f"Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆ{model}ï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n"
            "å¯¾å‡¦:\n"
            f"- ollama pull {model}\n"
            "- RAGï¼ˆPDFæ¤œç´¢ï¼‰ã¯ embedding ãƒ¢ãƒ‡ãƒ«ãŒç„¡ã„ã¨å‹•ãã¾ã›ã‚“"
        )


def _is_embedding_model_missing_error(e: Exception, model: str = "nomic-embed-text") -> bool:
    msg = str(e)
    low = msg.lower()
    return model in msg and ("è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“" in msg or "not found" in low or "model not found" in low)


@st.cache_resource(show_spinner=False)
def _build_vectorstore_from_paths(paths: list[str], signature: str):
    # signature ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼å®‰å®šåŒ–ã®ãŸã‚
    chunks = []
    splitter = _get_splitter()
    for p in paths:
        docs, loader_used = _load_pdf_docs_best_effort(p)
        docs = _normalize_docs_source(docs, os.path.basename(p))
        for d in docs:
            d.metadata = dict(d.metadata or {})
            d.metadata["loader"] = loader_used
            d.page_content = _clean_pdf_text(getattr(d, "page_content", "") or "")
        _strip_repeated_header_footer(docs)
        chunks.extend([c for c in splitter.split_documents(docs) if (c.page_content or "").strip()])
    _ensure_embedding_model("nomic-embed-text")
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    return Chroma.from_documents(documents=chunks, embedding=embeddings)


def _list_pdf_paths() -> list[str]:
    pdf_dir = os.environ.get("PDF_DIR", "./pdfs")
    pdf_path = os.environ.get("PDF_PATH", "vaccine_manual.pdf")
    paths: list[str] = []
    if pdf_path and os.path.exists(pdf_path) and pdf_path.lower().endswith(".pdf"):
        paths.append(pdf_path)
    try:
        if pdf_dir and os.path.isdir(pdf_dir):
            for f in sorted(os.listdir(pdf_dir)):
                if f.lower().endswith(".pdf"):
                    paths.append(os.path.join(pdf_dir, f))
    except Exception:
        pass
    # é‡è¤‡é™¤å»
    uniq: list[str] = []
    seen: set[str] = set()
    for p in paths:
        ap = os.path.abspath(p)
        if ap in seen:
            continue
        seen.add(ap)
        uniq.append(p)
    return uniq


def _signature(paths: list[str]) -> str:
    parts: list[str] = []
    for p in paths:
        try:
            st_ = os.stat(p)
            parts.append(f"{os.path.abspath(p)}|{int(st_.st_size)}|{float(st_.st_mtime)}")
        except Exception:
            parts.append(f"{os.path.abspath(p)}|NA|NA")
    return "\n".join(sorted(parts))

paths = _list_pdf_paths()
try:
    if not paths:
        st.error("å‚ç…§ã™ã‚‹PDFãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚`vaccine_manual.pdf` ã¾ãŸã¯ `./pdfs/` ã«PDFã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        st.stop()
    sig = _signature(paths)
    with st.spinner("PDFã‚’è§£æã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
        vectorstore = _build_vectorstore_from_paths(paths, sig)
    st.success(f"è³‡æ–™ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆ{len(paths)}ä»¶ï¼‰ã€‚")
except Exception as e:
    if _is_embedding_model_missing_error(e, "nomic-embed-text"):
        st.error(
            "Embeddingãƒ¢ãƒ‡ãƒ«ï¼ˆnomic-embed-textï¼‰ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\n\n"
            "å¯¾å‡¦:\n"
            "- ollama pull nomic-embed-text\n"
            "- RAGï¼ˆPDFæ¤œç´¢ï¼‰ã¯ embedding ãƒ¢ãƒ‡ãƒ«ãŒç„¡ã„ã¨å‹•ãã¾ã›ã‚“"
        )
    else:
        st.error(f"çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ã§ãã¾ã›ã‚“ã§ã—ãŸ: {e}")
    st.stop()

# ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
col1, col2 = st.columns([1, 3])
with col1:
    if st.button("å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.messages = []
        st.rerun()

# æ¨ªå±•é–‹ï¼ˆUXæœ€ä½é™ï¼‰: ä¾‹ãƒœã‚¿ãƒ³ / å†é€
if "queued_prompt" not in st.session_state:
    st.session_state.queued_prompt = ""
if "last_user_prompt" not in st.session_state:
    st.session_state.last_user_prompt = ""

quick_items = [
    "æ¥ç¨®å¾Œ7æ—¥é–“ã«è¨˜éŒ²ã™ã‚‹é …ç›®ã¯ï¼Ÿ",
    "37.5åº¦ä»¥ä¸Šã®ç™ºç†±ãŒå‡ºãŸã‚‰ã©ã†ã™ã‚Œã°ã„ã„ï¼Ÿ",
    "æ¥ç¨®éƒ¨ä½ã®è…«ã‚Œãƒ»ç—›ã¿ã¯ã©ã®ãã‚‰ã„ç¶šãï¼Ÿï¼ˆè³‡æ–™ã«ã‚ã‚‹ç¯„å›²ã§ï¼‰",
    "ç›¸è«‡å…ˆï¼ˆåŒ»ç™‚æ©Ÿé–¢/è‡ªæ²»ä½“/119ï¼‰ã®åˆ¤æ–­ã®ç›®å®‰ã¯ï¼Ÿ",
]

qcols = st.columns([1, 1, 1, 1])
for i, text in enumerate(quick_items):
    with qcols[i]:
        if st.button(f"ä¾‹: {text}", use_container_width=True):
            st.session_state.queued_prompt = text
            st.rerun()

rs_col1, rs_col2 = st.columns([1, 3])
with rs_col1:
    if st.button("å†é€", disabled=not bool(st.session_state.last_user_prompt)):
        st.session_state.queued_prompt = st.session_state.last_user_prompt
        st.rerun()
with rs_col2:
    if st.session_state.last_user_prompt:
        st.caption(f"æœ€å¾Œã®è³ªå•: {st.session_state.last_user_prompt}")

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and message.get("sources"):
            sources = message["sources"]
            locs = [str(s.get("location") or f"{s.get('source','è³‡æ–™')} {s.get('page_label','[P?]')}") for s in sources]
            st.markdown("**æ ¹æ‹ ï¼ˆå¼•ç”¨ï¼‰**: " + " / ".join(locs))
            with st.expander("æ ¹æ‹ ã®æŠœç²‹ã‚’è¡¨ç¤º"):
                for s in sources:
                    title = str(s.get("location") or f"{s.get('source','è³‡æ–™')} {s.get('page_label','[P?]')}")
                    st.markdown(f"- {title}")
                    if s.get("excerpt"):
                        st.code(str(s["excerpt"]))


def _extract_sources(docs):
    sources = []
    seen = set()
    for d in docs:
        meta = d.metadata or {}
        src = meta.get("source") or "è³‡æ–™"
        page = meta.get("page")
        page_num = page + 1 if isinstance(page, int) else None
        key = (src, page_num)
        if key in seen:
            continue
        seen.add(key)
        page_label = f"[P{page_num}]" if isinstance(page_num, int) else "[P?]"
        excerpt = _make_excerpt(d.page_content or "")
        sources.append(
            {
                "source": str(src),
                "page": page_num,
                "page_label": page_label,
                "excerpt": excerpt,
                "location": f"{src} {page_label}",
            }
        )
    return sources

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
prompt = st.chat_input("è³ªå•ã‚’ã©ã†ã")
if not prompt and st.session_state.queued_prompt:
    prompt = st.session_state.queued_prompt
    st.session_state.queued_prompt = ""

if prompt:
    st.session_state.last_user_prompt = prompt
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # RAGãƒ­ã‚¸ãƒƒã‚¯
    with st.chat_message("assistant"):
        try:
            with st.spinner("è³‡æ–™ã‚’ç¢ºèªä¸­..."):
                # æ¤œç´¢
                docs = vectorstore.similarity_search(prompt, k=k)
                context = "\n".join([doc.page_content for doc in docs])
                sources = _extract_sources(docs)

                # æ ¹æ‹ ãŒå–ã‚Œãªã„å ´åˆã¯ã€ç”Ÿæˆã›ãšã«å›ºå®šãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§è¿”ã™ï¼ˆæ–­å®š/hallucinationé˜²æ­¢ï¼‰
                if not sources:
                    answer = _no_sources_answer(prompt)
                else:
                    full_prompt = _build_answer_prompt(question=prompt, context=context)
                    response = ollama.generate(model=llm_model, prompt=full_prompt)
                    answer = response["response"]

            st.markdown(answer)
            if sources:
                locs = [str(s.get("location") or f"{s.get('source','è³‡æ–™')} {s.get('page_label','[P?]')}") for s in sources]
                st.markdown("**æ ¹æ‹ ï¼ˆå¼•ç”¨ï¼‰**: " + " / ".join(locs))
                with st.expander("æ ¹æ‹ ã®æŠœç²‹ã‚’è¡¨ç¤º"):
                    for s in sources:
                        title = str(s.get("location") or f"{s.get('source','è³‡æ–™')} {s.get('page_label','[P?]')}")
                        st.markdown(f"- {title}")
                        if s.get("excerpt"):
                            st.code(str(s["excerpt"]))

            st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})
        except Exception as e:
            st.error("ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚ã¾ãšã¯ Ollama / PDF / è¨­å®šï¼ˆkå€¤ãƒ»ãƒ¢ãƒ‡ãƒ«ï¼‰ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
            with st.expander("ãƒ­ã‚°å…¨æ–‡ï¼ˆå±•é–‹ï¼‰"):
                st.code(str(e))
            st.session_state.messages.append({"role": "assistant", "content": f"ã‚¨ãƒ©ãƒ¼: {e}"})