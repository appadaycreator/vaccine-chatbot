import streamlit as st
import ollama
import hashlib
import os
import tempfile
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings

# ãƒšãƒ¼ã‚¸ã®è¨­å®š
st.set_page_config(page_title="ãƒ¯ã‚¯ãƒãƒ³æ¥ç¨®å¾Œå¥åº·è¦³å¯Ÿã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ", page_icon="ğŸ¥")
st.title("ğŸ¥ å¥åº·è¦³å¯Ÿã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆ")
st.caption("åšåŠ´çœã®å®Ÿæ–½è¦é ˜ã«åŸºã¥ã„ãŸãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼ï¼ˆè¨­å®šï¼‰
with st.sidebar:
    st.header("è¨­å®š")
    llm_model = st.selectbox("å›ç­”ãƒ¢ãƒ‡ãƒ«", ["gemma2", "llama3.1"], index=0)
    k = st.slider("æ¤œç´¢ã®å¼·ã•ï¼ˆkå€¤ï¼‰", min_value=1, max_value=10, value=3, step=1)
    st.caption("åŸ‹ã‚è¾¼ã¿ãƒ¢ãƒ‡ãƒ«: `nomic-embed-text`ï¼ˆå›ºå®šï¼‰")

    uploaded_pdf = st.file_uploader("PDFã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ï¼ˆä»»æ„ï¼‰", type=["pdf"])
    st.caption("æœªã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰æ™‚ã¯ `vaccine_manual.pdf` ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")


def _normalize_docs_source(docs, source_label: str):
    for d in docs:
        d.metadata = dict(d.metadata or {})
        d.metadata["source"] = source_label
    return docs


@st.cache_resource(show_spinner=False)
def _build_vectorstore_from_path(pdf_path: str):
    loader = PyPDFLoader(pdf_path)
    docs = loader.load()
    docs = _normalize_docs_source(docs, os.path.basename(pdf_path))
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = text_splitter.split_documents(docs)
    embeddings = OllamaEmbeddings(model="nomic-embed-text")
    return Chroma.from_documents(documents=chunks, embedding=embeddings)


@st.cache_resource(show_spinner=False)
def _build_vectorstore_from_bytes(pdf_bytes: bytes, filename: str, file_hash_hex: str):
    # file_hash_hex ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼å®‰å®šåŒ–ã®ãŸã‚ï¼ˆbytesã ã‘ã§ã‚‚è‰¯ã„ãŒã€æ˜ç¤ºã—ã¦ãŠãï¼‰
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as f:
        f.write(pdf_bytes)
        tmp_path = f.name
    try:
        loader = PyPDFLoader(tmp_path)
        docs = loader.load()
        docs = _normalize_docs_source(docs, filename or "uploaded.pdf")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_documents(docs)
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        return Chroma.from_documents(documents=chunks, embedding=embeddings)
    finally:
        try:
            os.unlink(tmp_path)
        except OSError:
            pass

try:
    if uploaded_pdf is not None:
        pdf_bytes = uploaded_pdf.getvalue()
        pdf_hash = hashlib.sha256(pdf_bytes).hexdigest()
        with st.spinner("ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰PDFã‚’è§£æã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
            vectorstore = _build_vectorstore_from_bytes(pdf_bytes, uploaded_pdf.name, pdf_hash)
        st.success(f"è³‡æ–™ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆ{uploaded_pdf.name}ï¼‰ã€‚")
    else:
        with st.spinner("æ—¢å®šPDFã‚’è§£æã—ã¦çŸ¥è­˜ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...ï¼ˆåˆå›ã¯æ™‚é–“ãŒã‹ã‹ã‚Šã¾ã™ï¼‰"):
            vectorstore = _build_vectorstore_from_path("vaccine_manual.pdf")
        st.success("è³‡æ–™ã®èª­ã¿è¾¼ã¿ãŒå®Œäº†ã—ã¾ã—ãŸï¼ˆvaccine_manual.pdfï¼‰ã€‚")
except Exception as e:
    st.error(f"PDFã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {e}")
    st.stop()

# ãƒªã‚»ãƒƒãƒˆãƒœã‚¿ãƒ³
col1, col2 = st.columns([1, 3])
with col1:
    if st.button("å±¥æ­´ã‚’ãƒªã‚»ãƒƒãƒˆ"):
        st.session_state.messages = []
        st.rerun()

# ãƒãƒ£ãƒƒãƒˆå±¥æ­´ã®åˆæœŸåŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []

# å±¥æ­´ã®è¡¨ç¤º
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])
        if message["role"] == "assistant" and message.get("sources"):
            sources = message["sources"]
            pages = []
            for s in sources:
                if s.get("page") is not None:
                    pages.append(f"{s.get('source','è³‡æ–™')} p.{s['page']}")
                else:
                    pages.append(f"{s.get('source','è³‡æ–™')}")
            st.markdown("**æ ¹æ‹ ï¼ˆå‚ç…§ãƒšãƒ¼ã‚¸ï¼‰**: " + " / ".join(pages))
            with st.expander("æ ¹æ‹ ã®æŠœç²‹ã‚’è¡¨ç¤º"):
                for s in sources:
                    title = f"{s.get('source','è³‡æ–™')}"
                    if s.get("page") is not None:
                        title += f" p.{s['page']}"
                    st.markdown(f"- {title}")
                    if s.get("excerpt"):
                        st.caption(s["excerpt"])


def _extract_sources(docs):
    sources = []
    seen = set()
    for d in docs:
        meta = d.metadata or {}
        src = meta.get("source") or "è³‡æ–™"
        page = meta.get("page")
        page_num = page + 1 if isinstance(page, int) else None
        key = (src, page_num)
        if key in seen:
            continue
        seen.add(key)
        excerpt = (d.page_content or "").strip().replace("\n", " ")
        if len(excerpt) > 400:
            excerpt = excerpt[:400] + "â€¦"
        sources.append({"source": src, "page": page_num, "excerpt": excerpt})
    return sources

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
if prompt := st.chat_input("è³ªå•ã‚’ã©ã†ã"):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # RAGãƒ­ã‚¸ãƒƒã‚¯
    with st.chat_message("assistant"):
        with st.spinner("è³‡æ–™ã‚’ç¢ºèªä¸­..."):
            # æ¤œç´¢
            docs = vectorstore.similarity_search(prompt, k=k)
            context = "\n".join([doc.page_content for doc in docs])
            sources = _extract_sources(docs)
            
            # ç”Ÿæˆ
            full_prompt = f"""
ã‚ãªãŸã¯åšåŠ´çœã®è³‡æ–™ã«åŸºã¥ã„ã¦å›ç­”ã™ã‚‹å°‚é–€ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
ä»¥ä¸‹ã®ã€è³‡æ–™æŠœç²‹ã€‘ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ—¥æœ¬èªã§ç°¡æ½”ã«å›ç­”ã—ã¦ãã ã•ã„ã€‚
è³‡æ–™ã«è¨˜è¼‰ãŒãªã„å ´åˆã¯ã€Œè³‡æ–™å†…ã«ã¯è©²å½“ã™ã‚‹æƒ…å ±ãŒè¦‹å½“ãŸã‚Šã¾ã›ã‚“ã€ã¨ç­”ãˆã€è‡ªæ²»ä½“ã®ç›¸è«‡çª“å£ã¾ãŸã¯æ¥ç¨®ã‚’å—ã‘ãŸåŒ»ç™‚æ©Ÿé–¢ã¸ã®ç›¸è«‡ã‚’ä¿ƒã—ã¦ãã ã•ã„ã€‚

ã€è³‡æ–™æŠœç²‹ã€‘
{context}

è³ªå•: {prompt}
å›ç­”:
""".strip()
            response = ollama.generate(model=llm_model, prompt=full_prompt)
            answer = response['response']
            
            st.markdown(answer)
            if sources:
                pages = []
                for s in sources:
                    if s.get("page") is not None:
                        pages.append(f"{s.get('source','è³‡æ–™')} p.{s['page']}")
                    else:
                        pages.append(f"{s.get('source','è³‡æ–™')}")
                st.markdown("**æ ¹æ‹ ï¼ˆå‚ç…§ãƒšãƒ¼ã‚¸ï¼‰**: " + " / ".join(pages))
                with st.expander("æ ¹æ‹ ã®æŠœç²‹ã‚’è¡¨ç¤º"):
                    for s in sources:
                        title = f"{s.get('source','è³‡æ–™')}"
                        if s.get("page") is not None:
                            title += f" p.{s['page']}"
                        st.markdown(f"- {title}")
                        if s.get("excerpt"):
                            st.caption(s["excerpt"])

            st.session_state.messages.append({"role": "assistant", "content": answer, "sources": sources})